from matplotlib import pyplot
from sklearn import datasets
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import sys
import itertools
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix
#sys.stdout = open('/Users/robert/Desktop/VsCode/machine_learning/cs4210_finalProj/output.txt', 'w')

def plot_confusion_matrix(predicted_labels_list, y_test_list,class_names):
    cnf_matrix = confusion_matrix(y_test_list, predicted_labels_list)
    np.set_printoptions(precision=2)

    # Plot non-normalized confusion matrix
    plt.figure()
    generate_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix, without normalization')
    plt.show()

    # Plot normalized confusion matrix
    plt.figure()
    #generate_confusion_matrix(cnf_matrix, classes=class_names,
    generate_confusion_matrix(cnf_matrix,classes=class_names, normalize=True, title='Normalized confusion matrix')
#def generate_confusion_matrix(cnf_matrix, classes, normalize=False, title='Confusion matrix')
def generate_confusion_matrix(cnf_matrix,classes, normalize=False, title='Confusion matrix'):
    if normalize:
        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))
    plt.title(title)
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cnf_matrix.max() / 2.

    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
        plt.text(j, i, format(cnf_matrix[i, j], fmt), horizontalalignment="center",
                 color="white" if cnf_matrix[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

    return cnf_matrix
def cross_validation(model, _X, _y, _cv=10):
      '''Function to perform 10 Folds Cross-Validation
       
       Function returns the mean test_f1_score
      '''
      _scoring = ['accuracy', 'precision', 'recall', 'f1']
      results = cross_validate(estimator=model,
                               X=_X,
                               y=_y,
                               cv=_cv,
                               scoring=_scoring,
                               return_train_score = True)
      '''
      print("Mean Validation Accuracy", round(results['test_accuracy'].mean()*100,3), "%")
      print("Mean Validation Recall",round(results['test_recall'].mean()*100,3), "%")
      print ("Mean Validation Precision:", round (results['test_precision'].mean()*100,3),"%")
      print ("Mean Validation F1-score", round(results['test_f1'].mean()*100,3),"%") 
      print() 
      '''
      #return a list with all relavent metrics    
      return [round(results['test_f1'].mean()*100,5),round(results['test_accuracy'].mean()*100,5)
                ,round(results['test_recall'].mean()*100,5), round (results['test_precision'].mean(),5 )]
        




df = pd.read_csv("b_depressed.csv")
df['no_lasting_investment'].fillna(df['no_lasting_investment'].median(), inplace=True)
df.no_lasting_investment.isnull().sum()

dfDrop = df.drop(['Survey_id', 'Ville_id'], axis=1)
feature_names = list(dfDrop.columns)
feature_names = feature_names[:-1]
print("feature_names: " + str(feature_names))

#get the data into df for features and another for class labels (0- not depressed or 1)
X_db = dfDrop.iloc[:, :-1].values
y_db = dfDrop.iloc[:, -1].values

print("Starting machine learning phase ...")


n_estimators = [10,20, 30] #[100, 500, 1000]
criterion = ['gini', 'entropy']
max_depth = [3, 4, 6, 10]
max_features = ['sqrt', 'log2', None]
class_weight = ['balanced', 'balanced_subsample'] #to deal with the class imbalance problem

hyperparameters_list = []
best_params = []
max_f1_score = -1
acc_for_maxF1 = 0
recall_for_maxf1 = 0
prec_for_maxf1 = 0

for e in n_estimators:
    for c in criterion:
        for m in max_depth:
            for f in max_features:
                for w in class_weight:
                    RF_clf = RandomForestClassifier(n_estimators=e, criterion=c, max_depth=m, max_features=f, class_weight=w)
                    # this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before
                    rf_result = cross_validation(RF_clf, X_db, y_db, 10) 

                    #compare rf_result to max_f1_score and get the highest f1 score and save number of hyperparameters in a list
                    #print("rf_result)
                    print("f1-score:", rf_result[0])
                    if rf_result[0] > max_f1_score:
                        #save hyperparameters into list
                        hyperparameters_list.append([e, c, m, f, w])
                        print ("Mean Validation F1-score:",rf_result[0] ,"%") 
                        print("Mean Validation Accuracy:",rf_result[1] , "%")
                        print("Mean Validation Recall:",rf_result[2], "%")
                        print ("Mean Validation Precision:",rf_result[3] ,"%")
                        
                        
                        print("Hyperparameters: " + "n_estimators: " + str(e) + ",criterion: " + str(
                        c) + ",max_depth: " + str(m) + ", max_features: " + str(f) + ",class_weight: " + str(w))
                        print("Hyperparameters List: " + str(hyperparameters_list[-1]))
                        
                        #assign f1 and other metrics so we can print again at end
                        max_f1_score = rf_result[0]
                        acc_for_maxF1 = rf_result[1]
                        recall_for_maxf1 = rf_result[2]
                        prec_for_maxf1 = rf_result[3]

                    #print("Highest f1-score so far: " + str(max_f1_score))
                    #print("Hyperparameters: " + "n_estimators: " + str(e) + ",criterion: " + str(
                      #  c) + ",max_depth: " + str(m) + ", max_features: " + str(f) + ",class_weight: " + str(w))
                    #print("Hyperparameters List: " + str(hyperparameters_list[-1]))
best_params = hyperparameters_list[-1]
print("Best hyperparameters: " , best_params)
print ("Max mean Validation F1-score:",max_f1_score ,"%") 
print("Mean Validation Accuracy for max F1-score:",acc_for_maxF1 , "%")
print("Mean Validation Recall for max F1-score:",recall_for_maxf1, "%")
print ("Mean Validation Precision for max F1-score:",prec_for_maxf1 ,"%")
importance = [0] * 20 #set up importance, multiply by 20 since there are 20 features


kf = StratifiedKFold(n_splits = 10, shuffle=True)
rfc = RandomForestClassifier(n_estimators=best_params[0], criterion=best_params[1], max_depth=best_params[2],
 max_features=best_params[3], class_weight=best_params[4])
count = 1
#used to store the matrix -- average matrix from all k's
predicted_targets = np.array([])
actual_targets = np.array([])
# test data is not needed for fitting
for train_ix, test_ix in kf.split(X_db,y_db):
    train_x, train_y, test_x, test_y = X_db[train_ix], y_db[train_ix], X_db[test_ix], y_db[test_ix]

    rfc.fit(train_x, train_y)
    predicted_labels = rfc.predict(test_x)
    predicted_targets=np.append(predicted_targets,predicted_labels)
    actual_targets=np.append(actual_targets,test_y)

    # sort the feature index by importance score in descending order
    importances_index_desc = np.argsort(rfc.feature_importances_)[::-1]
    feature_labels = [feature_names[i] for i in importances_index_desc]
   
    for j in range(len(importance)):
        importance[j]+=rfc.feature_importances_[j]
    # plot
   
    plt.figure()
    plt.bar(feature_labels, rfc.feature_importances_[importances_index_desc])
    plt.xticks(feature_labels, rotation='vertical')
    plt.ylabel('Importance')
    plt.xlabel('Features')
    plt.title('Fold {}'.format(count))
    count+=1
    plt.tight_layout()
plt.tight_layout()
plt.show()

num_splits = 10
for k in range(len(importance)):
    importance[k] = importance[k] /num_splits
importance_and_idx = []

for i,v in enumerate(importance):
    #print('%0d Feature: %s, Score: %.5f' % (i, feature_names[i], v))
    importance_and_idx.append([feature_names[i], v])
#sort from most important to least
importance_and_idx.sort(key=lambda x: x[1],reverse=True)
print(importance_and_idx)
class_names = [ 'not_depressed', 'depressed' ]
#plotting the confusion matrix 
print(confusion_matrix(actual_targets, predicted_targets))
plot_confusion_matrix(predicted_targets, actual_targets, class_names)


#close after finsihing writing output to txt file
#sys.stdout.close()
"""
    Average importance values for features - Run corresponds to figs in slides 
"""
avg_importance_first_run = [['education_level', 0.12864775137844947], ['Age', 0.12070099134498666], ['lasting_investment', 0.11574672314924608],
                 ['no_lasting_investment', 0.09450960597798809], 
                ['living_expenses', 0.07888141044720574], ['durable_asset', 0.07497485958103829]]











